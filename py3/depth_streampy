import cv2
import numpy as np
import socket 
import threading
import struct
import time
import signal
from view_object_rec import process_frame

stop_event = threading.Event()

# Socket for live video feed
video_sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
video_sock.bind(('127.0.0.1', 8089))

# socket to send object loaction
location_sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)

latest_frame = None
lock = threading.Lock()

# Camera settings
baseline = 10 # In cm
focal_length = 386.16

stereo = cv2.StereoBM_create(numDisparities=16, blockSize=15)

def get_frames():
    global latest_frame
    while True:
        try:
            data, _ = video_sock.recvfrom(65536)
            size = struct.unpack("L", data[:4])[0]
            frame_data = data[4:size+4]

            frame = cv2.imdecode(np.frombuffer(frame_data, dtype=np.uint8), cv2.IMREAD_COLOR)

            if frame is None or frame.size == 0:
                print("Error: Received empty frame.")
                continue

            with lock:
                latest_frame = frame  

        except Exception as e:
            print(f"Error receiving frame: {e}")

def compute_depth(stereo_frame):
    # Split the stereo frame into left and right images
    height, width, _ = stereo_frame.shape
    left_image = stereo_frame[:, :width // 2]  # Left half
    right_image = stereo_frame[:, width // 2:]  # Right half

    # Convert images to grayscale
    gray_left = cv2.cvtColor(left_image, cv2.COLOR_BGR2GRAY)
    gray_right = cv2.cvtColor(right_image, cv2.COLOR_BGR2GRAY)

    gray_left = cv2.GaussianBlur(gray_left, (5, 5), 0)
    gray_right = cv2.GaussianBlur(gray_right, (5, 5), 0)

    # Stereo block matching for disparity map
    stereo = cv2.StereoBM_create(numDisparities=64, blockSize=21)
    disparity = stereo.compute(gray_left, gray_right)

    # Normalize disparity to avoid zero division
    disparity[disparity <= 0] = 1e-6
    disparity = np.clip(disparity, 1e-6, np.max(disparity))

    # Compute depth map (Depth = (focal_length * baseline) / disparity)
    depth_map = (focal_length * baseline) / disparity.astype(np.float32)
    depth_map = cv2.medianBlur(depth_map, 5)

    left_image_RGB = cv2.cvtColor(left_image, cv2.IMREAD_COLOR)

    return depth_map, left_image_RGB

def normalize_depth_map(depth_map):
    # Remove NaN and Inf values
    depth_map = np.nan_to_num(depth_map, nan=0, posinf=0, neginf=0)

    # Normalize safely: avoid division by zero by checking the max value
    max_val = np.max(depth_map)
    if max_val > 0:
        return depth_map / max_val
    else:
        return depth_map

def process_frames():
    global latest_frame
    while not stop_event.is_set():
        frame = None
        with lock:
            if latest_frame is not None:
                frame = latest_frame.copy()
            else:
                print("No frame available for processing.")
                time.sleep(0.1)
                continue

        depth_map, left_image = compute_depth(frame)

        object_center_left = process_frame(left_image)  

        if object_center_left:
            x_cen, y_cen = object_center_left
            # Get depth from the depth map at the center of the object (in left image)
            depth = depth_map[int(y_cen), int(x_cen)]
            depth_map = normalize_depth_map(depth_map)

            send_object_location(x_cen, y_cen, depth)

        # Display depth map
        if np.max(depth_map) > 0:
            depth_display = cv2.normalize(depth_map, None, 0, 255, cv2.NORM_MINMAX)
            depth_display = np.uint8(depth_display)
            cv2.imshow('Depth Map', depth_display)

        if cv2.waitKey(1) & 0xFF == ord('q'):
            break


def send_object_location(x_cen, y_cen, depth):
    try:
        # data = str(angle).encode()
        data =  f"{x_cen},{y_cen},{depth}".encode() 
        location_sock.sendto(data, ("127.0.0.1", 9090))
    except Exception as e:
        print(f"Error sending object location: {e}")
 

# Start threads for receiving and processing frames
receive_thread = threading.Thread(target=get_frames, daemon=True)
receive_thread.start()

process_thread = threading.Thread(target=process_frames, daemon=True)
process_thread.start()

print("Receiving and processing video...")

while True:
    if cv2.waitKey(1) & 0xFF == ord('q'): 
        print("Exiting... (q key pressed)")
        stop_event.set()  
        break

    if stop_event.is_set():
        break


receive_thread.join()
process_thread.join()

video_sock.close()
location_sock.close()
cv2.destroyAllWindows()
